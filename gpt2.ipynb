{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying GPT2 Locally\n",
    "This notebook will perform the first experiments on querying a small GPT2 model for personality traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch, json, tqdm, sys, random\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# GPT2 Generation\n",
    "model_name = 'gpt2' # <-- Change this per model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).eval()\n",
    "sent_checkpoints = ['distilbert-base-uncased-finetuned-sst-2-english']\n",
    "# sent_tokenizer = BartTokenizer.from_pretrained('textattack/facebook-bart-large-SST-2')\n",
    "# sent_model = BartForSequenceClassification.from_pretrained('textattack/facebook-bart-large-SST-2', num_labels=2)\n",
    "\n",
    "def logit_to_single_score(logits: torch.Tensor):\n",
    "    # Label 0 is negative, label 1 is positive\n",
    "    logits = logits[0]\n",
    "    logits = F.softmax(logits, dim=0)\n",
    "    logits = logits.cpu().detach().numpy()\n",
    "    neg_score = -1 * float(logits[0])\n",
    "    pos_score = float(logits[1])\n",
    "    return pos_score + neg_score\n",
    "\n",
    "def get_question_logits(question: str):\n",
    "    logits = []\n",
    "    for checkpoint in sent_checkpoints:\n",
    "        sent_model = AutoModelForSequenceClassification.from_pretrained(checkpoint).eval()\n",
    "        sent_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        inputs = sent_tokenizer(question, return_tensors=\"pt\")\n",
    "        outputs = sent_model(**inputs)\n",
    "        logits.append(outputs.logits)\n",
    "    return logits\n",
    "\n",
    "def compute_cosine(tensor_1: torch.Tensor, tensor_2: torch.Tensor):\n",
    "    # Compute the cosine between 2 tensors by computing their magnitudes and dot product\n",
    "    if len(tensor_1.shape) == 2:\n",
    "        tensor_1 = torch.squeeze(tensor_1)\n",
    "    if len(tensor_2.shape) == 2:\n",
    "        tensor_2 = torch.squeeze(tensor_2)\n",
    "    dot_prod = torch.dot(tensor_1, tensor_2)\n",
    "    mag_1 = torch.linalg.norm(tensor_1)\n",
    "    mag_2 = torch.linalg.norm(tensor_2)\n",
    "    cosine = (dot_prod / (mag_1 * mag_2)).detach().cpu().numpy()\n",
    "    return cosine\n",
    "\n",
    "def get_sent_score(q_logits: list[torch.Tensor], phrase: str):\n",
    "    scores = []\n",
    "    # Compute and compare logits for each model\n",
    "    for i, checkpoint in enumerate(sent_checkpoints):\n",
    "        question_logits = q_logits[i]\n",
    "        sent_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "        sent_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        inputs = sent_tokenizer(phrase, return_tensors=\"pt\")\n",
    "        outputs = sent_model(**inputs)\n",
    "        scores.append(compute_cosine(outputs.logits, question_logits))\n",
    "    \n",
    "    # Shift the mean score by the variance of the distribution\n",
    "    score = np.mean(scores) - np.std(scores)**2\n",
    "    return score\n",
    "    \n",
    "# Load the filtered questions\n",
    "with open('./data/questions.json', 'r') as f:\n",
    "    questions = json.load(f)\n",
    "# TODO: Output JSON?\n",
    "output_dict = {}\n",
    "# DEBUG: limit to single item\n",
    "questions = questions[0:1]\n",
    "for i, question in enumerate(tqdm.tqdm(questions)):\n",
    "    input_ids = tokenizer.encode(question['question'], return_tensors='pt')\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_length=len(input_ids[0]) + 16, \n",
    "        top_k=200, \n",
    "        top_p=0.95,\n",
    "        num_return_sequences=5\n",
    "    )\n",
    "\n",
    "    # Perform scoring and storing outputs\n",
    "    output_dict[i] = {}\n",
    "    output_dict[i]['question'] = question['text'].strip()\n",
    "    output_dict[i]['responses'] = []\n",
    "    logits = get_question_logits(question['text'].strip())\n",
    "\n",
    "    for sample_output in sample_outputs:\n",
    "        response_dict = {}\n",
    "        out_str = tokenizer.decode(sample_output, skip_special_tokens=True)[len(question['question']) - 1:]\n",
    "        out_str = out_str.split('\\n')[0]\n",
    "        response_dict['text'] = out_str\n",
    "        response_dict['score'] = get_sent_score(logits, out_str)\n",
    "        response_dict['facet'] = question['facet']\n",
    "        response_dict['domain'] = question['domain']\n",
    "        response_dict['reverse_score'] = question['reverse_score']\n",
    "        output_dict[i]['responses'].append(response_dict)\n",
    "\n",
    "with open(f\"./data/{model_name}-out.json\", 'w') as f:\n",
    "    json.dump(output_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d1be0de97a20210aae8b935cc650f6c5ecaa0dda6f4a7d35b61bad57d138ea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('psy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
