{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying GPT2 Locally\n",
    "This notebook will perform the first experiments on querying a small GPT2 model for personality traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch, json, tqdm, sys, random\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 1/1 [00:40<00:00, 40.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# GPT2 Generation\n",
    "model_name = 'gpt2' # <-- Change this per model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).eval()\n",
    "sent_checkpoints = ['distilbert-base-uncased-finetuned-sst-2-english', 'bhadresh-savani/distilbert-base-uncased-emotion']\n",
    "num_generations = 5\n",
    "\n",
    "def logit_to_single_score(logits: torch.Tensor):\n",
    "    # Label 0 is negative, label 1 is positive\n",
    "    logits = logits[0]\n",
    "    logits = F.softmax(logits, dim=0)\n",
    "    logits = logits.cpu().detach().numpy()\n",
    "    neg_score = -1 * float(logits[0])\n",
    "    pos_score = float(logits[1])\n",
    "    return pos_score + neg_score\n",
    "\n",
    "def get_question_logits(question: str):\n",
    "    logits = []\n",
    "    for checkpoint in sent_checkpoints:\n",
    "        sent_model = AutoModelForSequenceClassification.from_pretrained(checkpoint).eval()\n",
    "        sent_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        inputs = sent_tokenizer(question, return_tensors=\"pt\")\n",
    "        outputs = sent_model(**inputs)\n",
    "        logits.append(outputs.logits)\n",
    "    return logits\n",
    "\n",
    "def compute_cosine(tensor_1: torch.Tensor, tensor_2: torch.Tensor):\n",
    "    # Compute the cosine between 2 tensors by computing their magnitudes and dot product\n",
    "    if len(tensor_1.shape) == 2:\n",
    "        tensor_1 = torch.squeeze(tensor_1)\n",
    "    if len(tensor_2.shape) == 2:\n",
    "        tensor_2 = torch.squeeze(tensor_2)\n",
    "    dot_prod = torch.dot(tensor_1, tensor_2)\n",
    "    mag_1 = torch.linalg.norm(tensor_1)\n",
    "    mag_2 = torch.linalg.norm(tensor_2)\n",
    "    cosine = (dot_prod / (mag_1 * mag_2)).detach().cpu().numpy()\n",
    "    return cosine\n",
    "\n",
    "def get_sent_score(q_logits: list[torch.Tensor], phrase: str):\n",
    "    scores = []\n",
    "    # Compute and compare logits for each model\n",
    "    for i, checkpoint in enumerate(sent_checkpoints):\n",
    "        question_logits = q_logits[i]\n",
    "        sent_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "        sent_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        inputs = sent_tokenizer(phrase, return_tensors=\"pt\")\n",
    "        outputs = sent_model(**inputs)\n",
    "        scores.append(compute_cosine(outputs.logits, question_logits))\n",
    "    \n",
    "    # Shift the mean score by the variance of the distribution\n",
    "    score = np.mean(scores)\n",
    "    if score > 0.5:\n",
    "        score = max(0.5, score - np.std(scores)**2)\n",
    "    else:\n",
    "        score = min(0.5, score + np.std(scores)**2)\n",
    "\n",
    "    return score\n",
    "\n",
    "def check_ngram_overlap(base: str, target: str, n: int = 2):\n",
    "    \"\"\"\n",
    "    Returns true if there is an n-gram overlap (split by space) between the base and target string\n",
    "    \"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    base = tokenizer.encode(base.lower())\n",
    "    target = tokenizer.encode(target.lower())\n",
    "\n",
    "    for i in range(len(base) - n):\n",
    "        gram = base[i: i + n]\n",
    "        for j in range(len(target) - n):\n",
    "            target_gram = target[j: j + n]\n",
    "            overlap = True\n",
    "            for k in range(len(gram)):\n",
    "                if gram[k] != target_gram[k]:\n",
    "                    overlap = False\n",
    "            \n",
    "            if overlap:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Load the filtered questions\n",
    "with open('./data/questions.json', 'r') as f:\n",
    "    personality_test = json.load(f)\n",
    "questions = personality_test['questions']\n",
    "prompt = personality_test['prompt']\n",
    "output_dict = {}\n",
    "\n",
    "# DEBUG: limit to single item\n",
    "questions = questions[0:1]\n",
    "for i, question in enumerate(tqdm.tqdm(questions)):\n",
    "    # Initialize output structures\n",
    "    output_dict[i] = {}\n",
    "    output_dict[i]['question'] = question['text'].strip()\n",
    "    output_dict[i]['responses'] = []\n",
    "    logits = get_question_logits(question['text'].strip())\n",
    "    input_ids = tokenizer.encode(question['question'], return_tensors='pt')\n",
    "\n",
    "    while len(output_dict[i]['responses']) < num_generations:\n",
    "        # Generate only as much as needed to fill out the list\n",
    "        sample_outputs = model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True, \n",
    "            max_length=len(input_ids[0]) + 16, \n",
    "            top_k=200, \n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_generations - len(output_dict[i]['responses'])\n",
    "        )\n",
    "\n",
    "        # Perform scoring and storing outputs\n",
    "        for sample_output in sample_outputs:\n",
    "            response_dict = {}\n",
    "            # Get everything after the question\n",
    "            out_str = tokenizer.decode(sample_output, skip_special_tokens=True)[len(question['question']) - 1:]\n",
    "            out_str = out_str.split('\\n')[0]\n",
    "            # Continue if it began to generate the prompt\n",
    "            if check_ngram_overlap(prompt, out_str, n=2):\n",
    "                continue\n",
    "            # And break out if reached limit\n",
    "            if len( output_dict[i]['responses']) >= num_generations:\n",
    "                break\n",
    "            response_dict['text'] = out_str\n",
    "            response_dict['score'] = get_sent_score(logits, out_str)\n",
    "            response_dict['facet'] = question['facet']\n",
    "            response_dict['domain'] = question['domain']\n",
    "            response_dict['reverse_score'] = question['reverse_score']\n",
    "            output_dict[i]['responses'].append(response_dict)\n",
    "\n",
    "with open(f\"./data/{model_name}-out.json\", 'w') as f:\n",
    "    json.dump(output_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c5d588fe00534f0c4f86a7f79af4b032a899408d6af959f3a47a1b1075d36c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('psy': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
